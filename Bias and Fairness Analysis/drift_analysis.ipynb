{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "67226c9a-c830-4969-bb1e-9630cf9c90f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a840688-3a32-4cbe-93b2-e4f21d99c4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from pyspark.sql.functions import datediff, col\n",
    "from pyspark.sql.functions import col, explode_outer, lpad, regexp_replace\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import posexplode, concat, lit\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "os.environ['KAGGLEHUB_CACHE'] = '/Volumes/workspace/gjain/project_health'\n",
    "path = kagglehub.dataset_download(\"bonifacechosen/nhis-healthcare-claims-and-fraud-dataset\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a12f83e-a100-4da1-801a-f0b695ad1082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read into a dataframe\n",
    "raw = spark.read.csv(\"/Volumes/workspace/gjain/project_health/datasets/bonifacechosen/nhis-healthcare-claims-and-fraud-dataset/versions/3/simulated_healthcare_claims (1).csv\", header=True, inferSchema=True).withColumnRenamed(\"Amount Billed\", \"paid_amt\").withColumnRenamed(\"Fraud Type\", \"fraud\").withColumnRenamed(\"Date Admitted\", \"admission_dt\").withColumnRenamed(\"Date Discharged\", \"discharge_dt\").withColumnRenamed(\"Patient ID\", \"patient_id\").withColumnRenamed(\"Age\", \"age\").withColumnRenamed(\"Gender\", \"gender\")\n",
    "display(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298c165d-9db3-44b9-8c6e-e94105c8411b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Create age feature\n",
    "df = raw.toPandas()\n",
    "df['admission_dt'] = pd.to_datetime(df['admission_dt'])\n",
    "df['discharge_dt'] = pd.to_datetime(df['discharge_dt']) \n",
    "age_bins = [0, 18, 30, 45, 60, 75, 100]\n",
    "age_labels = ['0-17', '18-29', '30-44', '45-59', '60-74', '75+']\n",
    "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "\n",
    "# Feature engineering: use primary diagnosis/procedure codes, gender, LOS, etc.\n",
    "df['length_of_stay'] = (df['discharge_dt'] - df['admission_dt']).dt.days.astype('int') \n",
    "print(\"Number of null length_of_stay values:\", df['length_of_stay'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5ef225-61cd-4083-94b5-bfade8b16831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split input dataset into 2 datasets for drift analysis\n",
    "split_idx = len(df) // 2\n",
    "\n",
    "df1 = df.iloc[:split_idx, :].reset_index(drop=True)\n",
    "df2 = df.iloc[split_idx:, :].reset_index(drop=True)\n",
    "\n",
    "print(\"First half:\")\n",
    "print(len(df1))\n",
    "print(\"\\nSecond half:\")\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b906e807-47dd-4c3e-8b83-6bf38e8bf5a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "# Download the raw psi.py file from GitHub\n",
    "raw_url = \"https://raw.githubusercontent.com/mwburke/population-stability-index/refs/heads/master/psi.py\"\n",
    "response = requests.get(raw_url)\n",
    "\n",
    "with open(\"psi.py\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('psi', 'psi.py'); psi = importlib.util.module_from_spec(spec); spec.loader.exec_module(psi); calculate_psi = psi.calculate_psi\n",
    "\n",
    "result = calculate_psi(df1['age'],df2['age'],buckettype='quantiles', buckets=10, axis=1)  \n",
    "print(result)\n",
    "\n",
    "# Optional: remove the temporary file\n",
    "os.remove(\"psi.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade06c0a-0912-4274-ad43-a14d12e4548b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "rs = np.random.RandomState(5)\n",
    "\n",
    "initial = rs.normal(size = 100)\n",
    "new = rs.normal(loc = 0.2, size = 120)\n",
    "plot = sns.kdeplot(df1['age'], shade=True)\n",
    "plot = sns.kdeplot(df2['age'], shade=True)\n",
    "plot.set(yticklabels=[], xticklabels = [])\n",
    "sns.despine(left=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drift_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
