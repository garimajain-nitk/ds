{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67226c9a-c830-4969-bb1e-9630cf9c90f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install featexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a840688-3a32-4cbe-93b2-e4f21d99c4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from pyspark.sql.functions import datediff, col\n",
    "from pyspark.sql.functions import col, explode_outer, lpad, regexp_replace\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import posexplode, concat, lit\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a12f83e-a100-4da1-801a-f0b695ad1082",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769453277601}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read into a dataframe\n",
    "raw=spark.sql(\"\"\" select diagnosis.claim_id,diagnosis.patient_id,diagnosis.date_service,diagnosis.date_service_end,diagnosis_code,enrollment.patient_gender,patient_year_of_birth,sum(procedure.line_charge) as paid_amt \n",
    "from healthverity_claims_sample_patient_dataset.hv_claims_sample.diagnosis \n",
    "inner join healthverity_claims_sample_patient_dataset.hv_claims_sample.enrollment on diagnosis.patient_id = enrollment.patient_id\n",
    "inner join healthverity_claims_sample_patient_dataset.hv_claims_sample.procedure on diagnosis.patient_id = procedure.patient_id\n",
    "where admit_diagnosis_ind='Y'\n",
    "group by all\"\"\")\n",
    "raw = raw.withColumnRenamed(\"date_service\", \"admission_dt\").withColumnRenamed(\"date_service_end\", \"discharge_dt\").withColumnRenamed(\"diagnosis_code\",\"diag_cd\")\n",
    "display(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298c165d-9db3-44b9-8c6e-e94105c8411b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Create age feature\n",
    "df = raw.toPandas()\n",
    "df['admission_dt'] = pd.to_datetime(df['admission_dt'])\n",
    "df['discharge_dt'] = pd.to_datetime(df['discharge_dt']) \n",
    "\n",
    "# Feature engineering: use primary diagnosis/procedure codes, gender, LOS, etc.\n",
    "df['length_of_stay'] = (df['discharge_dt'] - df['admission_dt']).dt.days.astype('int') \n",
    "print(\"Number of null length_of_stay values:\", df['length_of_stay'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d160b6b2-71af-46b1-9fa2-af630008ea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from featexp import get_univariate_plots\n",
    "\n",
    "df_train = df\n",
    "# Call the function to generate univariate plots\n",
    "get_univariate_plots(df, \n",
    "                     'paid_amt', \n",
    "                     ['length_of_stay'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA_los_paidamt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
